Figure 7 shows how the sampling process works for a BATON
tree as illustrated in Figure 5. Node E collects statistics from its
routing fingers, which will recursively extract statistics from their
routing fingers. In this way, an information aggregation “tree” is
constructed (only the left-most subtree is displayed). This “tree”
is actually a graph as a node may have different parents. In an
iterative way, the global information will be aggregated at the root
node. But what is the expected statistics after some iterations?
We define two matrixes, R and P, to represent the relationship
between nodes.
R is a 1×N matrix. ri represents the corresponding local statistic
data of ith node in the network. P is an N × N matrix, representing
the routing finger relationship. If node j does not exist in the finger
table of node i, pi, j = 0. Otherwise, pi, j is a non-negative value
representing the weight of node j in node i’s finger table. Based
on Algorithm 7, pi, j = pi,k if both of them are non-negative. In
addition, the diagonal element pi,i is always non-negative and the
matrix P satisfies:
Let the number of non-negative elements in row i and column j be
pri and pcj respectively. In Chord, BATON or most other structured
overlays, pri and pcj are therefore O(log N).
Suppose all nodes perform Algorithm 7 to collect the statistics.
Let 1 × N matrix Ri represent the estimated statistics after i iterations.
That is, the element of row j denotes node j’s estimation. Rn
is represented as:
Theorem 5.1. The estimated statistics computed by Algorithm 7
converges to the approximate average global statistics of the network.
Proof. (sketch) In fact, we only need to prove that L = limn→∞Pn
exists and equals to a N × N matrix S where SR will produce a global 
statistics which approxi for each node
stochastic matrix, generated by considering nodes and their links
as a directed graph. In BATON network, each node can locate
arbitrary node in O(log N) step, which means that we can find a
constant c, the elements of matrix Pc are all non-zero. Hence, the
Perron-Frobenius theorem [9] guarantees that the limit L always
exists. And there is a stationary probability vector π that does not
change under application of the transition matrix. π is associated
with eigenvalue 1. That is π(P ? I) = 0. We get πP = π. The
element ai, j of L is equal to jth element of π. Because for each row
and column, there is a similar number of non-negative elements
with same value, vector π is approximate to ( 1N
Theorem 5.1 indicates the correctness of the sampling scheme
in a static environment (the initial statistics, matrix R, does not
change). In our scheme, we do not need to wait for the matrix
to converge. An approximate estimation is sufficient for building a
good range index. To handle the dynamism of the network, in Algorithm
7, we will recompute the global statistics periodically. The
new statistics are taken into account when new computation starts