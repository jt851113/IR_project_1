1. Sorted cumulative algorithm (SCA) SCA ?rstly sorts all objects in S in non-ascending order for each dimension and obtain d sorted array A1,A2,···,Ad. Thus, Ai[j].Di ? Ai[j + 1].Di, ?i,j ? [1,d]. Each round SCA starts by picking an undetermined object q ? S with the biggest sum on all dimensions and scan all the sorted arrays A1,A2,···,Ad separately. We use a counter array to keep track of the amount of dimensions that other objects outperform q. Speci?cally, SCA scans Ai from the top until encountering q and for each object p occurred earlier, namely p.Di > q.D i, ps corresponding cumulative counter p.counter is increased by one. Assume now SCA just ?nishes some round and q is the object to be determined andp ?{ S}/q. Obviously, we have the following theorems. Theorem 1 If p.counter ? k, thenp ?k q. Proof It is obviously p.counter records the amount of dimensions p outperforms q in this round. If p.counter ? k, according to De?nition 1, p ?k q. Theorem 2 If p.counter ? d?k, thenq ?k p. Proof If p.counter ? d?k, then it means that q outperforms p on more than k dimensions. According to De?nition 1, q ?k p. At the end of each round, SCA immediately can decide whether q ? DSKY (k,S,D) according to Theorem 1. Furthermore, applying Theorem 2, SCA earns the capability of pruning k-dominated objects (by q) early and therefore eliminate the search space signi?cantly. SCA terminates when no candidates in S is potential answer for k-dominant skyline query. Example 2 In our running example shown in Table 1(a), let k = 4, SCA ?rst sorts each object separately according to each dimension value and generates 6 arrays: A1 = {p3,p2,p6,p5,p1,p4}, A2 = {p4,p5,p3,p1,p2,p6}, A3 = {p4,p6,p1,p2,p3,p5}, A4 = {p6,p2,p4,p1,p5,p3}, A5 = {p2,p1,p5,p4,p6,p3}, A6 = {p4,p2,p1,p3,p5,p6}. Then with sum(p1)=p1.D1+p1.D2+p1.D3+p1.D4+p1.D5+p1.D6 = 102,sum(p2) = 142, sum(p3) = 105, sum(p4) = 150, sum(p5) = 95, and sum(p6) = 93, SCA picks out the object with biggest sum, that is p4, into the ?rst round. Since p4 is on the bottom of A1, then the counter array is [1,1,1,0,1,1] after scanning A1. And it remains same after the next two scan because p4 is on the top of A2 and A3. And as so on, after scanning A6, the counter array is [2,3,1,0,2,2]. According to this array, we immediately know that p1 outperforms p4 on 2 dimensions and p2 outperforms p4 on 3 dimensions, etc. Because there no object outperforms p4 on more than k(= 4) dimensions and p4 is output as a 4-dominant skyline object. Furthermore, the corresponding counters of p1,p3,p5 and p6 are all not greater than d?k = 2, according to Theorem 2, they are not in the answers and will not been examined further. Similarly, SCA picks p2 having the second biggest sum on all dimensions (and also the last undetermined object) and cumulate the counter array by scanning A1 ? A6. Finally, SCA outputs p2 as another answer and terminates since p1,p3,p5 and p6 have been pruned. It is important to process the object with high potential to k-dominate others as early as possible so that more k-dominated objects are pruned in the early stage. SCA employs the sum of values on all dimensions as the indicator on deciding which object should be explored ?rst. This criterion is reasonable since objects with bigger overall value sum tend to perform well on each dimension and therefore are more promising to k-dominate other objects. It is also relatively eficient since it only requires one sort on the dataset. The total time complexity of SCA is O(d·n·log n). 2. Parallel SCA (PSCA) As the large-scale repositories of information and knowledge in cooperative task dramatically develop, the distributed environment is becoming an increasingly dominant context for cooperative work to be carried out. In order to be practical towards real applications, an e?cient parallel algorithm for computing k-dominant skyline is of high interests. Here, we parallelize SCA to meet the requirement. Since SCA performs well on datasets of moderate size, parallelizing by decomposition is a viable track. The underlying idea is to divide the dataset into several partitions in moderate size so that SCA can be performed eficiently to obtain local answer, and then verify these local answers to retrieve the real answer in the original dataset. Parallel SCA (PSCA) ?rstly sorts the original dataset S using the sum of values on all dimensions in a non-ascending order and obtains a sorted objects list S. To support general progressive and load balancing, the original dataset should be partitioned into identical or at least close sizes. Suppose there are m nodes involved in the distributed computation, without loss of generality, we assume that S consists of km objects, then the ith, (m + i)th, (2m + i)th, ···, (( k?1)m + i)th object should be distributed to the same partition, denoted by Pi, wherei =1 ,···,m . After using SCA in each partition, PSCA puts the local kdominant skyline objects to other partitions and performs SCA again, to ?nish cross validation and obtains the ?nally global k-dominant skyline. Obviously, we have the following lemmas. Lemma 1 For Pi ? S, ifp ? DSKY (k,Pi,D), then
p ? DSKY (k,S,D). Lemma 2 A k-dominant skyline object p in the whole dataset S must be a k-dominant skyline object in any subset Pi ? S. The above lemmas are easily proved by contradiction. Using Lemma 1, we immediately know that any objects that are not local k-dominant skylines in the partitions cannot be in the global k-dominant skylines. Those points can be pruned. Lemma 2 guarantees the completeness of PSCA.

